library(readr)
abstracts <- read_csv("https://www.dropbox.com/sh/4uje3yw1hg79zaq/AADDeXFj7STJBgKPVxRTvuz4a?dl=1")
View(head(abstracts))
View(aabstracts[4:9])
View(abstracts[4:9])
View(abstracts[4:9,])
View(abstracts[10:20,])
abstracts <- read_csv("https://www.dropbox.com/s/5m3reylokvxcleo/asc_abstracts1419.csv?dl=1")
View(head(abstracts))
library(dplyr)
names(abstracts)
abstracts <- abstracts %>%
rename(id = X1,
abstract = 0)
abstracts <- abstracts %>%
rename(id = X1,
abstract = `0`)
names(abstracts)
abstracts <- abstracts %>%
rename(id = X1,
abstract = `0`) %>%
mutate(abstract = gsub("<p>", "", abstract),
abstract = gsub("</p>", "", abstract))
abstracts <- read_csv("https://www.dropbox.com/s/5m3reylokvxcleo/asc_abstracts1419.csv?dl=1")
abstracts <- abstracts %>%
rename(id = X1,
abstract = `0`) %>%
mutate(abstract = gsub("<p>", "", abstract),
abstract = gsub("</p>", "", abstract))
View(head(abstracts))
install.packages("keras")
library(keras)
install_keras()
y
install.packages(c("janeaustenr", "tokenizers"))
library(janeaustenr)
library(tokenizers)
max_length <- 40
text <- austen_books() %>%
filter(book == "Pride & Prejudice") %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
??str_c
library(stringr)
text <- austen_books() %>%
filter(book == "Pride & Prejudice") %>%
pull(text) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
text[1]
text[1:10]
print(sprintf("Corpus length: %d", length(text)))
## [1] "Corpus length: 684767"
chars <- text %>%
unique() %>%
sort()
print(sprintf("Total characters: %d", length(chars)))
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
??map
?purr::map
?purrr::map
library(purrr)
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
dataset[1]
dataset[5]
dataset <- transpose(dataset)
vectorize <- function(data, chars, max_length){
x <- array(0, dim = c(length(data$sentence), max_length, length(chars)))
y <- array(0, dim = c(length(data$sentence), length(chars)))
for(i in 1:length(data$sentence)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == data$sentence[[i]])
})
y[i,] <- as.integer(chars == data$next_char[[i]])
}
list(y = y,
x = x)
}
vectors <- vectorize(dataset, chars, max_length)
vectors[1]
text2 <- abstracts %>%
pull(abstract) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("Corpus length: %d", length(text)))
print(sprintf("Corpus length: %d", length(text2)))
## [1] "Corpus length: 684767"
chars <- text2 %>%
unique() %>%
sort()
print(sprintf("Total characters: %d", length(chars)))
text <- abstracts %>%
pull(abstract) %>%
str_c(collapse = " ") %>%
tokenize_characters(lowercase = FALSE, strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("Corpus length: %d", length(text)))
## [1] "Corpus length: 684767"
chars <- text %>%
unique() %>%
sort()
print(sprintf("Total characters: %d", length(chars)))
dataset <- map(
seq(1, length(text) - max_length - 1, by = 3),
~list(sentence = text[.x:(.x + max_length - 1)],
next_char = text[.x + max_length])
)
dataset <- transpose(dataset)
